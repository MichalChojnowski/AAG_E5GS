{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to help explore data, analyze performance drivers for our target variable, track and tune models using MLFlow, and package results to use for prediction.  This notebook is not exhaustive and not meant to capture all potential aspects of the modeling pipeline, but to provide a framework and guide to help generate predictive models to use for a whitespace analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import sweetviz as sv\n",
    "import mlflow\n",
    "\n",
    "sys.path.insert(0,'../../')\n",
    "\n",
    "from models.models import WhitespaceModel\n",
    "from utils.carto_helpers import set_creds, get_creds\n",
    "from constants.global_constants import auv_features, MLFLOW_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set carto credentials\n",
    "set_creds(type=\"cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will assign variables to core columns we will use for the rest of the analysis:\n",
    "- `store_table`: Enriched client locations with our dependent variable and aggregated predictor columns\n",
    "- `id_col`: ID column in `store_table`\n",
    "- `target`: Target column with dependent variable in `store_table`\n",
    "- `report_exclude`: List of columns to exclude from the analysis and EDA report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client store locations and revenue\n",
    "store_table = 'vtg_test_modeling_clean'\n",
    "\n",
    "# Variable assignment\n",
    "id_col = 'store_id'\n",
    "target = 'target_var'\n",
    "\n",
    "# Columns to exclude\n",
    "report_exclude = ['cartodb_id', 'the_geom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate whitespace models class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cartoframes as cf\n",
    "# import numpy as np\n",
    "\n",
    "# tbl=cf.read_carto(store_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf.to_carto(tbl.fillna(np.nan), store_table, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = WhitespaceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build out our train and test data.  The load data takes an input of:\n",
    "- `data_path`: Name of carto table stored in account (recommended) or path to local data file\n",
    "- `features`: Optional dictionary of features to include or exclude in our modeling dataset.  If not provided, uses the dictionary of features `auv_features` defined in `constants/global_constants.py`\n",
    "- `include`: Binary input to indicate if features list is designed to signify columns to include (1) or exclude (0) from our model dataset.  Defaults to 1\n",
    "- `catboost_pool`: Binary input to determine whether catboost pool should be returned as well as an item of `model_df` output\n",
    "- `test_size`: Optional column to determine the % to use for the validation dataset, defaults to 0.2.\n",
    "\n",
    "And returns two datasets:\n",
    "- `df`: Our dataset read in from Carto or our local file path\n",
    "- `model_df`: Dictionary object with train-test split and catboost pool if requested. Stored as model_df['X_train'], model_df['y_train'], model_df['X_test'], etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, model_df = wm.load_data(data_path=store_table, \n",
    "                            include=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis with helper functions to help get a sense for data quality and potential areas to explore further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`check_missing` takes our dataframe as input and provides a view on the number of columns with missing data as well as the most troublesome columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.check_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_corrs` takes as input our store dataframe, target column, and columns to exclude, and the number of rows to display and in turn outputs the top correlating features and the correlation coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.get_corrs(df, target, exclude_cols='cartodb_id', n_display=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell utilizes the sweetviz library to generate a profile report on each variable in our dataset and relations with the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_report = sv.analyze([df.drop(columns=report_exclude), \"Target Data\"], target_feat=target)\n",
    "# eda_report.show_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training helper functions have been created to utilize MLFlow tracking capabilities and enable fast testing and iteration.  This allows us to store different model hyperparameters and study effects on performance.\n",
    "\n",
    "Current functionality is designed to support xgboost, randomforest, and catboost regression models, but the scripts in `models.py` can be updated to incorporate more model types.  In the following cells, we train one model in each type and return the sklearn pipeline used to generate predictions and the resulting accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model type generates a sklearn pre-processing pipeline and model object.  First, we will instantiate our mlflow tracking uri.  To adjust this value, adjust the `MLFLOW_PATH` variable in `global_constants.py`.  This saves and stores mlflow runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment name, which must be unique and case sensitive\n",
    "experiment_id = mlflow.create_experiment(\"Initial Tests\")\n",
    "experiment = mlflow.get_experiment(experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are low due to the randomly generated target data, but we would shoot for an R2 of .2 or above, otherwise it is recommended to move to a business rules approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train xgboost model\n",
    "xgb, xgb_metrics = wm.mlflow_train(model_df, \"xgboost\", experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train random forest model\n",
    "# rf, rf_metrics = wm.mlflow_train(model_df, \"randomforest\", experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train catboost model\n",
    "cb, cb_metrics = wm.mlflow_train(model_df, \"catboost\", experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view a report on feature importance using the package SHAP and the following helper function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import hyperopt\n",
    "\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set up an experiment to optmize hyperparameters using MLFlow and Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 50\n",
    "METRIC = \"val_RMSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment name, which must be unique and case sensitive\n",
    "experiment_id = mlflow.create_experiment(\"Hyperopt\")\n",
    "experiment = mlflow.get_experiment(experiment_id)\n",
    "\n",
    "space = wm.search_space()\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objective = wm.hyperopt_objective(model_df, METRIC, experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperopt.fmin(fn=train_objective,\n",
    "              space=space,\n",
    "              algo=hyperopt.tpe.suggest,\n",
    "              max_evals=MAX_EVALS,\n",
    "              trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_best(experiment_id, METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine model results, run `mlflow ui` in terminal, within the directory where this notebook is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(cb, '../models/mod_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vantage] *",
   "language": "python",
   "name": "conda-env-vantage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
